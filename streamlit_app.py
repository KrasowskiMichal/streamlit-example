import streamlit as st
import pandas as pd
import altair as alt

@st.cache
def load_data():
    url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv"
    return pd.read_csv(url)

# load the dataset
df = load_data()

# title of the app
st.title("Demo Wizualizacji w Streamlit")

# selectbox for user to choose a origin of car
origin = st.selectbox(
    'Wybierz pochodzenie samochodu:',
    df['origin'].unique())

# filter dataframe based on user choice
df_filtered = df[df['origin'] == origin]

# display a scatter plot using altair
scatter_plot = alt.Chart(df_filtered).mark_circle().encode(
    x='horsepower',
    y='mpg',
    color='origin',
    tooltip=['name', 'horsepower', 'mpg']
).interactive()

st.altair_chart(scatter_plot, use_container_width=True)


# Generated by CodiumAI

import pytest

"""
Code Analysis

Objective:
The objective of the 'load_data' function is to load a dataset from a specified URL and return it as a pandas dataframe. The function is decorated with '@st.cache' which means that the function's output will be cached by Streamlit, allowing for faster access to the data in subsequent runs of the application.

Inputs:
The 'load_data' function does not take any inputs.

Flow:
The function first defines the URL of the dataset to be loaded. It then uses pandas' 'read_csv' function to load the dataset from the URL into a dataframe. Finally, the function returns the dataframe.

Outputs:
The main output of the 'load_data' function is a pandas dataframe containing the dataset loaded from the specified URL.

Additional aspects:
The '@st.cache' decorator used in the function means that the function's output will be cached by Streamlit, allowing for faster access to the data in subsequent runs of the application. This is particularly useful for large datasets that may take a long time to load.
"""

class TestLoadData:
    #  Tests that data can be loaded successfully from URL
    def test_load_data_success(self):
        # Arrange
        expected_columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'name']

        # Act
        data = load_data()

        # Assert
        assert isinstance(data, pd.DataFrame)
        assert data.columns.tolist() == expected_columns

    #  Tests that an error is raised when URL is not found
    def test_load_data_url_not_found(self, monkeypatch):
        # Arrange
        monkeypatch.setattr(pd, 'read_csv', lambda x: None)

        # Act & Assert
        with pytest.raises(Exception):
            load_data()

    #  Tests that an error is raised when CSV file is not found
    def test_load_data_csv_not_found(self, monkeypatch):
        # Arrange
        monkeypatch.setattr(pd, 'read_csv', lambda x: None)

    #  Tests that data can be read successfully from CSV file
    def test_load_data_csv_success(self, monkeypatch):
        # Arrange
        expected_columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'name']
        csv_data = pd.read_csv("https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv")
        monkeypatch.setattr(pd, 'read_csv', lambda x: csv_data)

    #  Tests that an error is raised when URL is invalid
    def test_load_data_invalid_url(self):
        # Arrange
        invalid_url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg_invalid.csv"

        # Act & Assert
        with pytest.raises(Exception):
            pd.read_csv(invalid_url)

    #  Tests that an error is raised when CSV file has an invalid format
    def test_load_data_invalid_csv_format(self, monkeypatch):
        # Arrange
        invalid_csv = pd.DataFrame({'A': [1, 2], 'B': [3, 4], 'C': ['a', 'b']})
        monkeypatch.setattr(pd, 'read_csv', lambda x: invalid_csv)

        # Act & Assert
        with pytest.raises(Exception):
            load_data()
